### 神经网络的优化

网络的优化分为几个方面：包括网络准确率的优化、网络速度的优化、模型大小的优化，有些时候我们最求其中的某一方面，有些时候我们需要综合考虑
各个方面，选取一个适合自己的方案。我们这里引用几个讲解详细的博客，加入我们自己的实际经验，记录一下我们在模型优化方面一些理解。

### 参考文献

[1] https://zhuanlan.zhihu.com/p/55846103

[2] https://github.com/Ewenwan/MVision/blob/master/CNN/Deep_Compression/readme.md

#### 1.准确率的优化

主要优化方法有：数据增强(Data Augmentation)，权重初始化(weight initialization)，随机梯度下降法（Stochastic Gradient descent），批规范化（Batch normalization) ，Dropout 等。

简单的思想，我们可以采用高准确率的模型，例如任务准确率在VGG16上不高，那我们可以采用Resnet50，densenet121等。这是一个再简单不过的做法，但是在特定模型下，针对具体任务，同样具有很多的优化方式。

模型合适的初始化参数设置，当我们从0开始训练一个模型的时候，选择合适的初始化参数方式，对后续的准确率具有一定影响，这个可以根据实验、经验测试选择。

加载预训练模型，例如我们在做OCR的时候，直接进行CTC文字识别，很多时候得到的准确率不高，但是如果我们采用imagenet预训练好的模型，进行文字识别的训练，往往效果会得到提升。

合适的学习率，通常训练我们初始学习率较大，然后逐渐降低学习率，防止过拟合，但是在有些模型下，并不完全这样操作。例如在Mnasnet的训练下，初始学习率从0到0.25做一个warming up，然后继续训练降低学习率。

不同框架也具有一定差异，例如resnet50，利用mxnet结合mixup，能够做到79%以上的准确率。所以大家在训练自己的模型的时候，要综合考虑多个方面，进行模型的优化。

#### 2.网络速度的优化

模型速度优化，我们这里只讨论前向计算的速度。在模型不变的情况下，需要对前向计算框架进行优化，包括利用SIMD、NEON等指令集加速，特殊的计算方式，如winnograd 卷积计算方式等。

在网络设计上，可以考虑高效的计算模型，如mobilenetv1/v2,shufflenetv1/v2,mnasnet等。这些网络中高效计算的block单元是我们值得参考借鉴的地方。这种网络在设计之初，就考虑了速度与准确率的平衡，这是高效网络设计的很有效的方式。

另外可以考虑采用剪枝，量化等方式，参考链接：

#### 3.模型大小的优化

模型压缩的主要分为在设计之初的模型优化，以及针对已有模型的压缩。
